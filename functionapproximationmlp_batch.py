# -*- coding: utf-8 -*-
"""FunctionApproximationMLP_Batch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K-b-iL9MtYPnqUltAV59FKDLHauMZ-k5
"""

import sys
import math
import random
import numpy as np
from matplotlib import cm
from numpy import exp,arange
import matplotlib.pyplot as plt
from numpy.random import randint, rand
from mpl_toolkits.mplot3d import Axes3D
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
from matplotlib.ticker import LinearLocator, FormatStrFormatter
from sklearn.model_selection import GridSearchCV, train_test_split
from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show

def humps(x):
    return 1/((x-0.3)**2+0.01) + 1/((x-0.9)**2+0.04) - 6

x_vals = np.arange(-5,5,0.1)
y_vals = humps(x_vals)
y_max = y_vals.max()
y_vals /= y_max

def z_func(x, y):
    return  (x**2 + y**2)*humps(x)#*(1/((x-0.3)**2+0.01) + 1/((x-0.9)**2+0.04) - 6)

x = np.arange(-5, 5, 0.1)
xy = [(j,k) for j in x for k in x]
out = [z_func(p[0],p[1]) for p in xy]

x_train, x_test, y_train, y_test = train_test_split(xy, out)

len(x_train)

# https://scikit-learn.org/stable/modules/neural_networks_supervised.html
# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html

def modelMLPRegressor(layers=[30], max_iter = 100):
    # set up network with parameters
    mlp = MLPRegressor(
        hidden_layer_sizes=layers,
        max_iter=max_iter,
        tol=0,
    )
    return mlp

def modelMLPRegressorBatch(layers=[30], max_iter = 100):
    # set up network with parameters
    mlp = MLPRegressor(
        hidden_layer_sizes=layers,
        max_iter=max_iter,
        batch_size = 1000,
        tol=0,
    )
    return mlp

"""# First State"""

layers=[30, 20]
max_iter = 100
mlp1 = modelMLPRegressor(layers=layers, max_iter=max_iter)
# train network
mlp1.fit(x_train,y_train)

# test
predictions = mlp1.predict(x_test)

mse1 = mean_squared_error(y_test, predictions)
print(f"mse1={mse1}")

print(f"mlp1.score={mlp1.score(x_test, y_test)}")
print(f"mlp1 Loss={mlp1.loss_}")
print(f"mlp1 Best Loss is ={mlp1.best_loss_}")
plt.plot(mlp1.loss_curve_)
plt.title("Loss Curve in SGD", fontsize=14)
plt.xlabel('Iterations')
plt.ylabel('Training Loss')
plt.show()

layers=[30, 20]
max_iter = 100
mlpb1 = modelMLPRegressorBatch(layers=layers, max_iter=max_iter)
# train network
mlpb1.fit(x_train,y_train)

# test
predictions = mlpb1.predict(x_test)

mseb1 = mean_squared_error(y_test, predictions)
print(f"mseb1={mseb1}")

print(f"mlpb1.score={mlpb1.score(x_test, y_test)}")
print(f"mlpb1 Loss={mlpb1.loss_}")
print(f"mlpb1 Best Loss is ={mlpb1.best_loss_}")
plt.plot(mlpb1.loss_curve_)
plt.title("Loss Curve in BGD", fontsize=14)
plt.xlabel('Iterations')
plt.ylabel('Training Loss')
plt.show()

len(mlp1.coefs_)

mlp1.coefs_[0]

np.where(abs(mlp1.coefs_[2])> 50)

mlp1.coefs_[2]

layers = np.arange(10, 110, 10)
layers

layers = layers[::-1]
print(layers)

"""# Second State"""

layers=[30, 20, 10]
max_iter = 500
mlp2 = modelMLPRegressor(layers=layers, max_iter=max_iter)
# train network
mlp2.fit(x_train,y_train)

# test
predictions = mlp2.predict(x_test)

mse2 = mean_squared_error(y_test, predictions)
print(f"mse2={mse2}")

print(f"mlp2.score={mlp2.score(x_test, y_test)}")
print(f"mlp2 Loss={mlp2.loss_}")
print(f"mlp2 Best Loss is ={mlp2.best_loss_}")
plt.plot(mlp2.loss_curve_)
plt.title("Loss Curve", fontsize=14)
plt.xlabel('Iterations')
plt.ylabel('Training Loss')
plt.show()

"""# Third State"""

layers=[30]
max_iter = 100
mlp3 = modelMLPRegressor(layers=layers, max_iter=max_iter)
# train network
mlp3.fit(x_train,y_train)

# test
predictions = mlp3.predict(x_test)

mse3 = mean_squared_error(y_test, predictions)
print(f"mse3={mse3}")

print(f"mlp3.score={mlp3.score(x_test, y_test)}")
print(f"mlp3 Loss={mlp3.loss_}")
print(f"mlp3 Best Loss is ={mlp3.best_loss_}")
plt.plot(mlp3.loss_curve_)
plt.title("Loss Curve", fontsize=14)
plt.xlabel('Iterations')
plt.ylabel('Training Loss')
plt.show()

"""# Forth State"""

layers=[30]
max_iter = 1000
mlp4 = modelMLPRegressor(layers=layers, max_iter=max_iter)
# train network
mlp4.fit(x_train,y_train)

# test
predictions = mlp4.predict(x_test)

mse4 = mean_squared_error(y_test, predictions)
print(f"mse4={mse4}")

print(f"mlp4.score={mlp4.score(x_test, y_test)}")
print(f"mlp4 Loss={mlp4.loss_}")
print(f"mlp4 Best Loss is ={mlp4.best_loss_}")
plt.plot(mlp4.loss_curve_)
plt.title("Loss Curve", fontsize=14)
plt.xlabel('Iterations')
plt.ylabel('Training Loss')
plt.show()

"""# Fifth State"""

layers=[60]
max_iter = 100
mlp5 = modelMLPRegressor(layers=layers, max_iter=max_iter)
# train network
mlp5.fit(x_train,y_train)

# test
predictions = mlp5.predict(x_test)

mse5 = mean_squared_error(y_test, predictions)
print(f"mse5={mse5}")

print(f"mlp5.score={mlp5.score(x_test, y_test)}")
print(f"mlp5 Loss={mlp5.loss_}")
print(f"mlp5 Best Loss is ={mlp5.best_loss_}")
plt.plot(mlp5.loss_curve_)
plt.title("Loss Curve", fontsize=14)
plt.xlabel('Iterations')
plt.ylabel('Training Loss')
plt.show()

def MLPModelBatch(hidden_layer_sizes, iter_number, x_train, y_train, x_test, y_test, batch_size = 100):

    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,
                       batch_size = batch_size)

    losses = []
    test_performance = []

    for epoch in range(iter_number):
        # print(f"epoch={epoch}")
        # Make 100 passes over the batches

        # for batch in range(500, 7501, 500):
            # Perform partial fits on batches of 500 examples

            # Simulate batches, these could also be loaded from `.npy`
            # X_train_batch = X_train[batch-500:batch]
            # y_train_batch = y_train[batch-500:batch]

        # regr.partial_fit(x_train, y_train)
        mlp.partial_fit(x_train, y_train)

        losses.append(mlp.loss_)
        test_performance.append(mlp.score(x_test, y_test))

    predictions = mlp.predict(x_test)
    mse = mean_squared_error(y_test, predictions)

    return losses, test_performance, mse, mlp.loss_, mlp.best_loss_, mlp.loss_curve_

def MLPModel(hidden_layer_sizes, iter_number, x_train, y_train, x_test, y_test):

    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,
                       batch_size = 1)

    losses = []
    test_performance = []

    for epoch in range(iter_number):
        # print(f"epoch={epoch}")
        # Make 100 passes over the batches

        # for batch in range(500, 7501, 500):
            # Perform partial fits on batches of 500 examples

            # Simulate batches, these could also be loaded from `.npy`
            # X_train_batch = X_train[batch-500:batch]
            # y_train_batch = y_train[batch-500:batch]

        # regr.partial_fit(x_train, y_train)
        mlp.partial_fit(x_train, y_train)

        losses.append(mlp.loss_)
        test_performance.append(mlp.score(x_test, y_test))

    predictions = mlp.predict(x_test)
    mse = mean_squared_error(y_test, predictions)

    return losses, test_performance, mse, mlp.loss_, mlp.best_loss_, mlp.loss_curve_

iter_number = 500
batch_size = len(x_train)

"""# First State"""

hidden_layer_sizes = [30, 20]
losses_1, test_performance_1, mse_1, loss_1, best_loss_1, loss_curve_1 = MLPModel(hidden_layer_sizes, iter_number,
                                                                                     x_train, y_train, x_test, y_test)
print(f"for model 1: mlp.score={test_performance_1[-1]}, mse={mse_1}, loss_={loss_1}, best_loss={best_loss_1}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
fig = plt.figure(figsize=(10,5))
ax1.title.set_text("Training Loss for SGD")
ax2.title.set_text("Score on Validation set for SGD")
ax1.plot(range(len(losses_1)), losses_1)
ax2.plot(range(len(test_performance_1)), test_performance_1)
plt.show()

hidden_layer_sizes = [30, 20]
losses_b_1, test_performance_b_1, mse_b_1, loss_b_1, best_loss_b_1, loss_curve_b_1 = MLPModelBatch(hidden_layer_sizes, iter_number,
                                                                                                   x_train, y_train, x_test, y_test, batch_size = batch_size)
print(f"for model 1: mlp.score={test_performance_b_1[-1]}, mse={mse_b_1}, loss_={loss_b_1}, best_loss={best_loss_b_1}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
fig = plt.figure(figsize=(10,5))
ax1.title.set_text("Training Loss for BGD")
ax2.title.set_text("Score on Validation set for BGD")
ax1.plot(range(len(losses_b_1)), losses_b_1)
ax2.plot(range(len(test_performance_b_1)), test_performance_b_1)
plt.show()

hidden_layer_sizes = [30, 20]
losses_b32_1, test_performance_b32_1, mse_b32_1, loss_b32_1, best_loss_b32_1, loss_curve_b32_1 = MLPModelBatch(hidden_layer_sizes, iter_number,
                                                                                                   x_train, y_train, x_test, y_test, batch_size = 32)
print(f"for model 1: mlp.score={test_performance_b32_1[-1]}, mse={mse_b32_1}, loss_={loss_b32_1}, best_loss={best_loss_b32_1}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
fig = plt.figure(figsize=(10,5))
ax1.title.set_text("Training Loss for BGD")
ax2.title.set_text("Score on Validation set for MiniBatchGD 32")
ax1.plot(range(len(losses_b32_1)), losses_b32_1)
ax2.plot(range(len(test_performance_b32_1)), test_performance_b32_1)
plt.show()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
fig = plt.figure(figsize=(10,5))

ax1.title.set_text("Compare Loss")
ax1.plot(range(len(losses_1)), losses_1, label = "SGD")
ax1.plot(range(len(losses_b_1)), losses_b_1, label = "BGD")
ax1.plot(range(len(losses_b32_1)), losses_b32_1, label = "MiniBatchGD 32")
ax1.legend(loc="upper right")

ax2.title.set_text("Compare Score on Validation set")
ax2.plot(range(len(test_performance_1)), test_performance_1, label = "SGD")
ax2.plot(range(len(test_performance_b_1)), test_performance_b_1, label = "BGD")
ax2.plot(range(len(test_performance_b32_1)), test_performance_b32_1, label = "MiniBatchGD 32")
ax2.legend(loc="lower right")

# x = range(1, len(test_performance_1)+1)
# plt.figure(figsize=(10,8))
# plt.plot(x, test_performance_1, label = "SGD")
# plt.plot(x, test_performance_b_1, label = "BGD")
# plt.xlabel("Iterations")
# plt.ylabel("Performance")
# plt.title('Compare Performance of SGD and BGD')
# plt.legend()
# plt.show()

"""# Second State"""

hidden_layer_sizes = [30, 20, 10]
losses_2, test_performance_2, mse_2, loss_2, best_loss_2, loss_curve_2 = MLPModel(hidden_layer_sizes, iter_number,
                                                                                     x_train, y_train, x_test, y_test)
print(f"for model 2: mlp.score={test_performance_2[-1]}, mse={mse_2}, loss_={loss_2}, best_loss={best_loss_2}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_2)), losses_2)
ax2.plot(range(len(test_performance_2)), test_performance_2)
plt.show()

hidden_layer_sizes = [30, 20, 10]
losses_b_2, test_performance_b_2, mse_b_2, loss_b_2, best_loss_b_2, loss_curve_b_2 = MLPModelBatch(hidden_layer_sizes, iter_number,
                                                                                                   x_train, y_train, x_test, y_test, batch_size = batch_size)
print(f"for model 2: mlp.score={test_performance_b_2[-1]}, mse={mse_b_2}, loss_={loss_b_2}, best_loss={best_loss_b_2}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_b_2)), losses_b_2)
ax2.plot(range(len(test_performance_b_2)), test_performance_b_2)
plt.show()

hidden_layer_sizes = [30, 20, 10]
losses_b32_2, test_performance_b32_2, mse_b32_2, loss_b32_2, best_loss_b32_2, loss_curve_b32_2 = MLPModelBatch(hidden_layer_sizes, iter_number,
                                                                                                   x_train, y_train, x_test, y_test, batch_size = 32)
print(f"for model 2: mlp.score={test_performance_b32_2[-1]}, mse={mse_b32_2}, loss_={loss_b32_2}, best_loss={best_loss_b32_2}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_b32_2)), losses_b32_2)
ax2.plot(range(len(test_performance_b32_2)), test_performance_b32_2)
plt.show()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
fig = plt.figure(figsize=(10,5))

ax1.title.set_text("Compare Loss")
ax1.plot(range(len(losses_2)), losses_2, label = "SGD")
ax1.plot(range(len(losses_b_2)), losses_b_2, label = "BGD")
ax1.plot(range(len(losses_b32_2)), losses_b32_2, label = "MiniBatchGD 32")
ax1.legend(loc="upper right")

ax2.title.set_text("Compare Score on Validation set")
ax2.plot(range(len(test_performance_2)), test_performance_2, label = "SGD")
ax2.plot(range(len(test_performance_b_2)), test_performance_b_2, label = "BGD")
ax2.plot(range(len(test_performance_b32_2)), test_performance_b32_2, label = "MiniBatchGD 32")
ax2.legend(loc="lower right")

"""# Third State"""

hidden_layer_sizes = [30]
losses_3, test_performance_3, mse_3, loss_3, best_loss_3, loss_curve_3 = MLPModel(hidden_layer_sizes, iter_number,
                                                                                     x_train, y_train, x_test, y_test)
print(f"for model 3: mlp.score={test_performance_3[-1]}, mse={mse_3}, loss_={loss_3}, best_loss={best_loss_3}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_3)), losses_3)
ax2.plot(range(len(test_performance_3)), test_performance_3)
plt.show()

hidden_layer_sizes = [30]
losses_b_3, test_performance_b_3, mse_b_3, loss_b_3, best_loss_b_3, loss_curve_b_3 = MLPModelBatch(hidden_layer_sizes, iter_number,
                                                                                                   x_train, y_train, x_test, y_test, batch_size = batch_size)
print(f"for model 3: mlp.score={test_performance_b_3[-1]}, mse={mse_b_3}, loss_={loss_b_3}, best_loss={best_loss_b_3}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_b_3)), losses_b_3)
ax2.plot(range(len(test_performance_b_3)), test_performance_b_3)
plt.show()

hidden_layer_sizes = [30]
losses_b32_3, test_performance_b32_3, mse_b32_3, loss_b32_3, best_loss_b32_3, loss_curve_b32_3 = MLPModelBatch(hidden_layer_sizes, iter_number,
                                                                                                   x_train, y_train, x_test, y_test, batch_size = 32)
print(f"for model 3: mlp.score={test_performance_b32_3[-1]}, mse={mse_b32_3}, loss_={loss_b32_3}, best_loss={best_loss_b32_3}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_b32_3)), losses_b32_3)
ax2.plot(range(len(test_performance_b32_3)), test_performance_b32_3)
plt.show()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
fig = plt.figure(figsize=(10,5))

ax1.title.set_text("Compare Loss")
ax1.plot(range(len(losses_3)), losses_3, label = "SGD")
ax1.plot(range(len(losses_b_3)), losses_b_3, label = "BGD")
ax1.plot(range(len(losses_b32_3)), losses_b32_3, label = "MiniBatchGD 32")
ax1.legend(loc="upper right")

ax2.title.set_text("Compare Score on Validation set")
ax2.plot(range(len(test_performance_3)), test_performance_3, label = "SGD")
ax2.plot(range(len(test_performance_b_3)), test_performance_b_3, label = "BGD")
ax2.plot(range(len(test_performance_b32_3)), test_performance_b32_3, label = "MiniBatchGD 32")
ax2.legend(loc="lower right")

"""# Forth State"""

# hidden_layer_sizes = [30]
# losses_4, test_performance_4, mse_4, loss_4, best_loss_4, loss_curve_4 = MLPModel(hidden_layer_sizes, iter_number = 2000,
#                                                                                      x_train, y_train, x_test, y_test)
# print(f"for model 4: mlp.score={test_performance_4[-1]}, mse={mse_4}, loss_={loss_4}, best_loss={best_loss_4}")
# # Plotting results:
# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
# ax1.title.set_text("Training Loss")
# ax2.title.set_text("Score on Validation set")
# ax1.plot(range(len(losses_4)), losses_4)
# ax2.plot(range(len(test_performance_4)), test_performance_4)
# plt.show()

# hidden_layer_sizes = [30]
# losses_b_4, test_performance_b_4, mse_b_4, loss_b_4, best_loss_b_4, loss_curve_b_4 = MLPModelBatch(hidden_layer_sizes, iter_number = 2000,
#                                                                                                    x_train, y_train, x_test, y_test, batch_size = batch_size)
# print(f"for model 4: mlp.score={test_performance_b_4[-1]}, mse={mse_b_4}, loss_={loss_b_4}, best_loss={best_loss_b_4}")
# # Plotting results:
# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
# ax1.title.set_text("Training Loss")
# ax2.title.set_text("Score on Validation set")
# ax1.plot(range(len(losses_b_4)), losses_b_4)
# ax2.plot(range(len(test_performance_b_4)), test_performance_b_4)
# plt.show()

# hidden_layer_sizes = [30]
# losses_b32_4, test_performance_b32_4, mse_b32_4, loss_b32_4, best_loss_b32_4, loss_curve_b32_4 = MLPModelBatch(hidden_layer_sizes, iter_number = 2000,
#                                                                                                    x_train, y_train, x_test, y_test, batch_size = 32)
# print(f"for model 4: mlp.score={test_performance_b32_4[-1]}, mse={mse_b32_4}, loss_={loss_b32_4}, best_loss={best_loss_b32_4}")
# # Plotting results:
# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
# ax1.title.set_text("Training Loss")
# ax2.title.set_text("Score on Validation set")
# ax1.plot(range(len(losses_b32_4)), losses_b32_4)
# ax2.plot(range(len(test_performance_b32_4)), test_performance_b32_4)
# plt.show()

# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
# fig = plt.figure(figsize=(10,5))

# ax1.title.set_text("Compare Loss")
# ax1.plot(range(len(losses_4)), losses_4, label = "SGD")
# ax1.plot(range(len(losses_b_4)), losses_b_4, label = "BGD")
# ax1.plot(range(len(losses_b32_4)), losses_b32_4, label = "MiniBatchGD 32")
# ax1.legend(loc="upper right")

# ax2.title.set_text("Compare Score on Validation set")
# ax2.plot(range(len(test_performance_4)), test_performance_4, label = "SGD")
# ax2.plot(range(len(test_performance_b_4)), test_performance_b_4, label = "BGD")
# ax2.plot(range(len(test_performance_b32_4)), test_performance_b32_4, label = "MiniBatchGD 32")
# ax2.legend(loc="lower right")



"""# Fifth State"""

hidden_layer_sizes = [60]
losses_5, test_performance_5, mse_5, loss_5, best_loss_5, loss_curve_5 = MLPModel(hidden_layer_sizes, iter_number,
                                                                                     x_train, y_train, x_test, y_test)
print(f"for model 5: mlp.score={test_performance_5[-1]}, mse={mse_5}, loss_={loss_5}, best_loss={best_loss_5}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_5)), losses_5)
ax2.plot(range(len(test_performance_5)), test_performance_5)
plt.show()

hidden_layer_sizes = [60]
losses_b_5, test_performance_b_5, mse_b_5, loss_b_5, best_loss_b_5, loss_curve_b_5 = MLPModelBatch(hidden_layer_sizes, iter_number,
                                                                                                   x_train, y_train, x_test, y_test, batch_size = batch_size)
print(f"for model 5: mlp.score={test_performance_b_5[-1]}, mse={mse_b_5}, loss_={loss_b_5}, best_loss={best_loss_b_5}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_b_5)), losses_b_5)
ax2.plot(range(len(test_performance_b_5)), test_performance_b_5)
plt.show()

hidden_layer_sizes = [60]
losses_b32_5, test_performance_b32_5, mse_b32_5, loss_b32_5, best_loss_b32_5, loss_curve_b32_5 = MLPModelBatch(hidden_layer_sizes, iter_number,
                                                                                                   x_train, y_train, x_test, y_test, batch_size = 32)
print(f"for model 5: mlp.score={test_performance_b32_5[-1]}, mse={mse_b32_5}, loss_={loss_b32_5}, best_loss={best_loss_b32_5}")
# Plotting results:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
ax1.title.set_text("Training Loss")
ax2.title.set_text("Score on Validation set")
ax1.plot(range(len(losses_b32_5)), losses_b32_5)
ax2.plot(range(len(test_performance_b32_5)), test_performance_b32_5)
plt.show()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))
fig = plt.figure(figsize=(10,5))

ax1.title.set_text("Compare Loss")
ax1.plot(range(len(losses_5)), losses_5, label = "SGD")
ax1.plot(range(len(losses_b_5)), losses_b_5, label = "BGD")
ax1.plot(range(len(losses_b32_5)), losses_b32_5, label = "MiniBatchGD 32")
ax1.legend(loc="upper right")

ax2.title.set_text("Compare Score on Validation set")
ax2.plot(range(len(test_performance_5)), test_performance_5, label = "SGD")
ax2.plot(range(len(test_performance_b_5)), test_performance_b_5, label = "BGD")
ax2.plot(range(len(test_performance_b32_5)), test_performance_b32_5, label = "MiniBatchGD 32")
ax2.legend(loc="lower right")



"""# Compare different Models"""

x = range(1, len(losses_1)+1)
plt.figure(figsize=(10,8))
plt.plot(x, losses_1, label = "Model 1")
plt.plot(x, losses_2, label = "Model 2")
plt.plot(x, losses_3, label = "Model 3")
# plt.plot(range(991, len(losses_4)+1), losses_4[990:], label = "Loss Model 4")
plt.plot(x, losses_5, label = "Model 5")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.title('Compare Loss in SGD for different Models')
plt.legend()
plt.show()

x = range(1, len(losses_1)+1)
plt.figure(figsize=(10,8))
plt.plot(x, losses_b_1, label = "Model 1")
plt.plot(x, losses_b_2, label = "Model 2")
plt.plot(x, losses_b_3, label = "Model 3")
# plt.plot(range(991, len(losses_4)+1), losses_4[990:], label = "Loss Model 4")
plt.plot(x, losses_b_5, label = "Model 5")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.title('Compare Loss in BGD for different Models')
plt.legend()
plt.show()

x = range(1, len(losses_1)+1)
plt.figure(figsize=(10,8))
plt.plot(x, losses_b32_1, label = "Model 1")
plt.plot(x, losses_b32_2, label = "Model 2")
plt.plot(x, losses_b32_3, label = "Model 3")
# plt.plot(range(991, len(losses_4)+1), losses_4[990:], label = "Loss Model 4")
plt.plot(x, losses_b32_5, label = "Model 5")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.title('Compare Loss in MiniBatchGD 32 for different Models')
plt.legend()
plt.show()

x = range(1, len(test_performance_1)+1)
plt.figure(figsize=(10,8))
plt.plot(x, test_performance_1, label = "Model 1")
plt.plot(x, test_performance_2, label = "Model 2")
plt.plot(x, test_performance_3, label = "Model 3")
# plt.plot(range(991, len(losses_4)+1), losses_4[990:], label = "Loss Model 4")
plt.plot(x, test_performance_5, label = "Model 5")
plt.xlabel("Iterations")
plt.ylabel("Performance")
plt.title('Compare Performance of different Models in SGD')
plt.legend()
plt.show()

x = range(1, len(test_performance_1)+1)
plt.figure(figsize=(10,8))
plt.plot(x, test_performance_b_1, label = "Model 1")
plt.plot(x, test_performance_b_2, label = "Model 2")
plt.plot(x, test_performance_b_3, label = "Model 3")
# plt.plot(range(991, len(losses_4)+1), losses_4[990:], label = "Loss Model 4")
plt.plot(x, test_performance_b_5, label = "Model 5")
plt.xlabel("Iterations")
plt.ylabel("Performance")
plt.title('Compare Performance of different Models in BGD')
plt.legend()
plt.show()

x = range(1, len(test_performance_1)+1)
plt.figure(figsize=(10,8))
plt.plot(x, test_performance_b32_1, label = "Model 1")
plt.plot(x, test_performance_b32_2, label = "Model 2")
plt.plot(x, test_performance_b32_3, label = "Model 3")
# plt.plot(range(991, len(losses_4)+1), losses_4[990:], label = "Loss Model 4")
plt.plot(x, test_performance_b32_5, label = "Model 5")
plt.xlabel("Iterations")
plt.ylabel("Performance")
plt.title('Compare Performance of different Models in MiniBatchGD 32')
plt.legend()
plt.show()

